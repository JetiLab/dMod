% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/toolsSeverin.R
\name{distributed_computing}
\alias{distributed_computing}
\title{Run any R function on a remote HPC system with SLURM}
\usage{
distributed_computing(
  ...,
  jobname,
  partition = "single",
  cores = 16,
  nodes = 1,
  mem_per_core = 2,
  walltime = "01:00:00",
  ssh_passwd = NULL,
  machine = "cluster",
  var_values = NULL,
  no_rep = NULL,
  recover = TRUE,
  purge_local = FALSE,
  compile = FALSE,
  link = FALSE,
  custom_folders = NULL,
  resetSeeds = TRUE,
  returnAll = TRUE
)
}
\arguments{
\item{...}{R code to be remotely executed. Parameters to be changed for each run
must be named \code{var_i} (see "Details").}

\item{jobname}{Unique name (character) for the run. Existing runs with the same
name will be overwritten. Must not contain the string "Minus".}

\item{partition}{SLURM partition name to use. Default is \code{"single"}.}

\item{cores}{Number of cores per node. Values above 16 may limit available nodes.}

\item{nodes}{Number of nodes per task. Default is 1; typically should not be changed.}

\item{mem_per_core}{Memory per CPU core in GB. Default is 2 GB.}

\item{walltime}{Maximum runtime in format \code{"hh:mm:ss"}. Default is 1 hour.}

\item{ssh_passwd}{Password string for SSH authentication via \code{sshpass}.
Optional, and only used if key-based authentication is unavailable.}

\item{machine}{SSH address of the remote HPC system, e.g. \code{"user@cluster"}.}

\item{var_values}{List of parameter arrays. Each array corresponds to one variable
\code{var_i}. The length of each array determines the number of SLURM array jobs.
Mutually exclusive with \code{no_rep}.}

\item{no_rep}{Integer number of repetitions (mutually exclusive with \code{var_values}).}

\item{recover}{Logical; if \code{TRUE}, no computation is performed. Instead,
the returned list of functions \code{check()}, \code{get()}, and \code{purge()}
can be used to interact with previously submitted jobs.}

\item{purge_local}{Logical; if \code{TRUE}, the \code{purge()} function also
deletes local result files.}

\item{compile}{Logical; if \code{TRUE}, all C/C++ source files (\verb{*.c}, \verb{*.cpp})
are transferred to the cluster and fully recompiled into shared objects (\code{.so}).
If set to \code{TRUE}, this overrides \code{link = TRUE}.}

\item{link}{Logical; if \code{TRUE}, only existing object files (\verb{*.o}) are
transferred to the cluster and linked into shared objects (\code{.so}),
skipping compilation. If no \code{.o} files are found, an error is raised.
This option is ignored if \code{compile = TRUE}.}

\item{custom_folders}{Named vector with exactly three elements: \code{"compiled"},
\code{"output"}, and \code{"tmp"}. Each value is a relative path specifying where
compiled files, temporary data, and output results should be stored.
If \code{NULL}, all operations occur in the current working directory.}

\item{resetSeeds}{Logical; if \code{TRUE} (default), removes \code{.Random.seed}
from the transferred workspace to ensure each node has independent random seeds.}

\item{returnAll}{Logical; if \code{TRUE} (default), retrieves all remote files.
If \code{FALSE}, only result files (\verb{*result.RData}) are fetched.}
}
\value{
A list containing three functions:
\itemize{
\item \code{check()} – Checks whether all remote results are complete.
\item \code{get()} – Downloads results and loads them into
\code{cluster_result} in the local workspace.
\item \code{purge()} – Deletes temporary remote files; optionally removes local ones.
}
}
\description{
Generates R and bash scripts, transfers them to a remote HPC system via SSH,
and executes the given R code in parallel using the SLURM batch manager.
The function handles workspace export, job submission, and result retrieval.
}
\details{
\code{distributed_computing()} generates R and bash scripts designed to run
on an HPC system managed by SLURM. The current R workspace together with the
scripts are exported and transferred to the remote system via SSH.
If ssh-key authentication is not possible, the SSH password can be provided and
is used by \code{sshpass} (which must be installed locally).

The code to be executed remotely is passed to the \code{...} argument; its final
output is stored in \code{cluster_result}, which can be loaded in the local
workspace via the \code{get()} function.

It is possible to run multiple repetitions of the same program (via \code{no_rep})
or to pass a list of parameter arrays through \code{var_values}. Parameters that
vary between runs must be named \code{var_i}, where \emph{i} matches the index
of the corresponding array in \code{var_values}.
}
\examples{
\dontrun{
out_distributed_computing <- distributed_computing(
{
  mstrust(
    objfun=objective_function,
    center=outer_pars,
    studyname = "study",
    rinit = 1,
    rmax = 10,
    fits = 48,
    cores = 16,
    iterlim = 700,
    sd = 4
  )
},
jobname = "my_name",
partition = "single",
cores = 16,
nodes = 1,
mem_per_core = 2,
walltime = "02:00:00",
ssh_passwd = "password",
machine = "cluster",
var_values = NULL,
no_rep = 20,
recover = F,
compile = F,
link = F
)
out_distributed_computing$check()
out_distributed_computing$get()
out_distributed_computing$purge()
result <- cluster_result
print(result)


# calculate profiles
var_list <- profile_pars_per_node(best_fit, 4)
profile_jobname <- paste0(fit_filename,"_profiles_opt")
method <- "optimize"
profiles_distributed_computing <- distributed_computing(
  {
    profile(
      obj = obj,
      pars =  best_fit,
      whichPar = (as.numeric(var_1):as.numeric(var_2)),
      limits = c(-5, 5),
      cores = 16,
      method = method,
      stepControl = list(
        stepsize = 1e-6,
        min = 1e-4, 
        max = Inf, 
        atol = 1e-2,
        rtol = 1e-2, 
        limit = 100
      ),
      optControl = list(iterlim = 20)
    )
  },
  jobname = profile_jobname,
  partition = "single",
  cores = 16,
  nodes = 1,
  walltime = "02:00:00",
  ssh_passwd = "password",
  machine = "cluster",
  var_values = var_list,
  no_rep = NULL,
  recover = F,
  compile = F,
  link = F
)
profiles_distributed_computing$check()
profiles_distributed_computing$get()
profiles_distributed_computing$purge()
profiles  <- NULL
for (i in cluster_result) {
  profiles <- rbind(profiles, i)
}
}

}
